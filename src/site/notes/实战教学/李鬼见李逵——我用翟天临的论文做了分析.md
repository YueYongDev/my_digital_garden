---
{"title":"李鬼见李逵——我用翟天临的论文做了分析","categories":"实战教学","tags":["数据分析"],"abbrlink":"9f29","dg-publish":true,"permalink":"/实战教学/李鬼见李逵——我用翟天临的论文做了分析/","dgPassFrontmatter":true}
---


![](https://cdn.ytools.xyz/uPic/1240-20230116111558438.jpeg)

> 完整源码可在公众号：「01 二进制」后台回复：「翟天临」获取

昨天是元宵节，在南京，元宵节一到也意味着这个年过完了，我们也该回到自己的工作岗位上了。都说今年的瓜特别多（葫芦娃的那种），但是过年期间最甜的我想非翟天临的“**知网是什么？**”莫属了吧。

前段时间，微博上开始不断爆出翟天临学术不端，论文抄袭的消息，以至于牵扯到其导师、院长甚至整个北京电影学院。

![](https://cdn.ytools.xyz/uPic/1240-20230116111619222.jpeg)

我平常不怎么关注娱乐圈，所以刚开始并没有把这件事放在心上，直到网上爆出翟的论文大篇幅抄袭陈坤论文的消息，我才对这位娱乐圈博士的文章起了兴趣。接下来就让我们以一个 coder 的角度来硬核分析下翟的论文吧。

## 实验环境

工欲善其事，必先利其器，在开始分析之前，我先说明此次分析所处的实验环境，以免出现异常：

- MacOS 10.14.3
- Python 3.6.8（Anaconda）
- Visual Studio Code
- 使用的包有：
  - pkuseg（分词）
  - matplotlib（绘图）
  - wordcloud（词云）
  - numpy（数学计算）
  - sklearn（机器学习）

## 数据获取

说实话，起初我以为就算翟不知“知网”为何物，“知网”也该收录翟的文章吧，可我在知网搜了好久也没能找到翟的论文，好在我在今日头条上找到了他的文章，保存在`data/zhai.txt`中。说到这，还真要感谢翟天临啊，都是因为他，大家才变得这么有学术精神，开始研究起本科硕士博士论文了。

![](https://cdn.ytools.xyz/uPic/1240-20230116111625177.jpeg)

## 数据清理

上一节我们已经将他的论文保存到一个 txt 中了，所以我们需要先将文章加载到内存中：

```python
# 数据获取（从文件中读取）
def readFile(file_path):
    content = []
    with open(file_path, encoding="utf-8") as f:
        content = f.read()
    return content
```

我统计了下，除去开头的标题和末尾的致谢，总共 25005 个字。

接下来我们来进行数据清理，在这里我用了**pkuseg**对内容进行分词处理，同时去掉停用词后输出分词的结果。

> 所谓停用词就是在语境中没有具体含义的文字，例如这个、那个，你我他，的得地，以及标点符合等等。因为没人在搜索的时候去用这些没意义的停用词搜索，为了使得分词效果更好，我就要把这些停用词过滤掉。

```python
# 数据清理（分词和去掉停用词）
def cleanWord(content):
    # 分词
    seg = pkuseg.pkuseg()
    text = seg.cut(content)

    # 读取停用词
    stopwords = []
    with open("stopwords/哈工大停用词表.txt", encoding="utf-8") as f:
        stopwords = f.read()

    new_text = []
    # 去掉停用词
    for w in text:
        if w not in stopwords:
            new_text.append(w)

    return new_text
```

执行结果：

![](https://cdn.ytools.xyz/uPic/1240-20230116111641435.jpeg)

### 这里我提两点：

1. 为什么分词工具用的是**pkuseg**而不是**jieba**？

**pkuseg**是北大推出的一个分词工具，官方地址是：https://github.com/lancopku/pkuseg-python，他的README中说他是目前中文分词工具中效果最好的。

![](http://upload-images.jianshu.io/upload_images/5666077-bd5ee379d6fffcd5.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

2. 为什么用哈工大的停用词表？

停用词表的下载地址在：https://github.com/YueYongDev/stopwords。以下是几个常用停用词表的对比：

| 停用词表         | 效果较好的文本种类 |
| ---------------- | ------------------ |
| 哈工大停用词表   | 文献期刊类文本     |
| 百度停用词表     | 新闻报道类文本     |
| 四川大学停用词表 | 邮件文献类文本     |

> 参考文献：官琴, 邓三鸿, 王昊. 中文文本聚类常用停用词表对比研究[J]. 数据分析与知识发现, 2006, 1(3).
>
> 有兴趣阅读此篇论文的可在公众号：「01 二进制」后台回复：「停用词表对比研究」获取

## 数据统计

说是数据统计，其实也没什么好统计的，这里简单化一下，就是统计下各个词出现的频率，然后输出词频最高的 15 个词

```python
# 数据整理（统计词频）
def statisticalData(text):
    # 统计每个词的词频
    counter = Counter(text)
    # 输出词频最高的15个单词
    pprint.pprint(counter.most_common(15))
```

打印的结果：

![](https://cdn.ytools.xyz/uPic/1240-20230116111648233.jpeg)

> 真的是个不可多得的“好演员”啊，能将角色带入生活，即使肚中无货却仍用自己的表演能力为自己设立一个“学霸”人设，人物形象如此饱满，兴许这就是创作的艺术吧！

文章中说的最多的就是生活、角色、人物、性格这些词，这些正是一个好演员的精神所在，如果我们将这些词做成**词云**的话，可能效果会更好。

### 生成词云

词云生成这个部分我采用的是**wordcloud**库，使用起来非常简单，网上教程也有很多，这里需要提一点的就是：为了防止中文乱码情况的发生，需要配置**font_path**这个参数。中文字体可以选用系统的，也可以网上找，这里我推荐一个免费的中文字体下载的网址：http://www.lvdoutang.com/zh/0/0/1/1.html

下面是生成词云的代码：

```python
# 数据可视化（生成词云）
def drawWordCloud(text, file_name):
    wl_space_split = " ".join(text)

    # 设置词云背景图
    b_mask = plt.imread('assets/img/bg.jpg')
    # 设置词云字体（若不设置则无法显示中文）
    font_path = 'assets/font/FZZhuoYTJ.ttf'
    # 进行词云的基本设置（背景色，字体路径，背景图片，词间距）
    wc = WordCloud(background_color="white",font_path=font_path, mask=b_mask, margin=5)
    # 生成词云
    wc.generate(wl_space_split)
    # 显示词云
    plt.imshow(wc)
    plt.axis("off")
    plt.show()
    # 将词云图保存到本地
    path = os.getcwd()+'/output/'
    wc.to_file(path+file_name)
```

![](https://cdn.ytools.xyz/uPic/1240-20230116111654020.jpeg)

## 真假李逵（文章对比）

分析完了“李鬼”，我们有必要请出他的真身“李逵”兄弟了，同样还是和之前一样的套路，先找到数据，然后分词统计词频，这里就不重复操作了，直接放出词云图。

![](https://cdn.ytools.xyz/uPic/1240-20230116111707023.jpeg)

看到这图是不是觉得和翟的词云图异常相似，那么，这“真假李逵”之间到底有多像呢？接下来我们来计算下两篇文章的相似度吧。

### 文章相似度比较

#### TF-IDF

文章相似度的比较有很多种方法，使用的模型也有很多类别，包括**TF-IDF,LDA,LSI**等，这里方便起见，就只使用 TF-IDF 来进行比较了。

![](https://cdn.ytools.xyz/uPic/1240-20230116111714329.jpeg)

TF-IDF 实际上就是在词频 TF 的基础上再加入 IDF 的信息，IDF 称为逆文档频率，不了解的可以看下阮一峰老师的讲解：<https://www.ruanyifeng.com/blog/2013/03/tf-idf.html>，里面对 TFIDF 的讲解也是十分透彻的。

#### Sklearn

scikit-learn 也简称 sklearn, 是机器学习领域当中最知名的 Python 模块之一，官方地址为：https://github.com/scikit-learn/scikit-learn，其包含了很多种机器学习的方式，下面我们借助于 Sklearn 中的模块**TfidfVectorizer**来计算两篇文章之间的相似度，代码如下：

```python
# 计算文本相似度
def calculateSimilarity(s1, s2):
    def add_space(s):
            return ' '.join(cleanWord(s))

    # 将字中间加入空格
    s1, s2 = add_space(s1), add_space(s2)
    # 转化为TF矩阵
    cv = TfidfVectorizer(tokenizer=lambda s: s.split())
    corpus = [s1, s2]
    vectors = cv.fit_transform(corpus).toarray()
    # 计算TF系数
    return np.dot(vectors[0], vectors[1]) / (norm(vectors[0]) * norm(vectors[1]))
```

> 除了 Sklearn，我们还可以使用**gensim**调用一些模型进行计算，考虑到文章篇幅，就由读者自己去搜集资料实现吧。

我们将翟的论文和陈的论文分别传入该函数后，输出结果为：

```
两篇文章的相似度为：
0.7074857881770839
```

其实这个结果我还是挺意外的，只知道这“李鬼”长得像，却没想到相似度竟然高达 70.7%。当然，作为弟弟，翟的这个事那都不是个事。🙈

![](http://upload-images.jianshu.io/upload_images/5666077-781ddda619dbf8f1.gif?imageMogr2/auto-orient/strip)

> 完整源码可在公众号：「01 二进制」后台回复：「翟天临」获取

![](https://cdn.ytools.xyz/uPic/1240-20230116111817587.jpeg)
